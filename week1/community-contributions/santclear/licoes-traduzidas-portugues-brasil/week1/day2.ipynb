{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d15d8294-3328-4e07-ad16-8a03e9bbfdb9",
   "metadata": {},
   "source": [
    "# Bem-vindo ao Lab do Dia 2!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada885d9-4d42-4d9b-97f0-74fbbbfe93a9",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/resources.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#f71;\">Antes de come√ßarmos --</h2>\n",
    "            <span style=\"color:#f71;\">Achei que valia a pena indicar rapidamente esta p√°gina com recursos √∫teis para o curso. Ela inclui links para todos os slides.<br/>\n",
    "            <a href=\"https://edwarddonner.com/2024/11/13/llm-engineering-resources/\">https://edwarddonner.com/2024/11/13/llm-engineering-resources/</a><br/>\n",
    "            Por favor, mantenha este link nos favoritos; continuarei adicionando mais links √∫teis por l√° com o tempo.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ffe36f",
   "metadata": {},
   "source": [
    "## Primeiro - vamos falar sobre a API de Chat Completions\n",
    "\n",
    "1. A maneira mais simples de chamar um LLM\n",
    "2. Ela se chama Chat Completions porque est√° dizendo: \"aqui est√° uma conversa, por favor, preveja o que deve vir em seguida\"\n",
    "3. A API de Chat Completions foi inventada pela OpenAI, mas √© t√£o popular que todo mundo usa!\n",
    "\n",
    "### Vamos come√ßar chamando a OpenAI novamente - mas n√£o se preocupem, pessoal sem OpenAI, a vez de voc√™s est√° chegando!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e38f17a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chave de API encontrada e, at√© aqui, parece tudo certo!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if not api_key:\n",
    "    print(\"Nenhuma chave de API foi encontrada - v√° at√© o notebook de solu√ß√£o de problemas nesta pasta para identificar e corrigir!\")\n",
    "elif not api_key.startswith(\"sk-proj-\"):\n",
    "    print(\"Uma chave de API foi encontrada, mas ela n√£o come√ßa com sk-proj-; verifique se voc√™ est√° usando a chave correta - veja o notebook de solu√ß√£o de problemas\")\n",
    "else:\n",
    "    print(\"Chave de API encontrada e, at√© aqui, parece tudo certo!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97846274",
   "metadata": {},
   "source": [
    "## Voc√™ sabe o que √© um Endpoint?\n",
    "\n",
    "Se n√£o souber, revise o guia Technical Foundations na pasta guides\n",
    "\n",
    "E aqui est√° um endpoint que pode interessar a voc√™...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5af5c188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'gpt-5-nano',\n",
       " 'messages': [{'role': 'user', 'content': 'Conte-me um fato divertido'}]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "headers = {\"Authorization\": f\"Bearer {api_key}\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "payload = {\n",
    "    \"model\": \"gpt-5-nano\",\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"Conte-me um fato divertido\"}]\n",
    "}\n",
    "\n",
    "payload\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d0ab242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-CY022UrhaIMGhjCLTbkPK7rhNDff5',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1762219310,\n",
       " 'model': 'gpt-5-nano-2025-08-07',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': 'Fato divertido: o polvo tem tr√™s cora√ß√µes. Dois deles bombeiam sangue para as br√¢nquias, enquanto o terceiro bombeia para o resto do corpo. E o sangue dele √© azul, por causa da hemocianina (cujo cobre ajuda a transportar oxig√™nio). Quer mais uma curiosidade?',\n",
       "    'refusal': None,\n",
       "    'annotations': []},\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 11,\n",
       "  'completion_tokens': 778,\n",
       "  'total_tokens': 789,\n",
       "  'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0},\n",
       "  'completion_tokens_details': {'reasoning_tokens': 704,\n",
       "   'audio_tokens': 0,\n",
       "   'accepted_prediction_tokens': 0,\n",
       "   'rejected_prediction_tokens': 0}},\n",
       " 'service_tier': 'default',\n",
       " 'system_fingerprint': None}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.post(\n",
    "    \"https://api.openai.com/v1/chat/completions\",\n",
    "    headers=headers,\n",
    "    json=payload\n",
    ")\n",
    "\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb11a9f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fato divertido: o polvo tem tr√™s cora√ß√µes. Dois deles bombeiam sangue para as br√¢nquias, enquanto o terceiro bombeia para o resto do corpo. E o sangue dele √© azul, por causa da hemocianina (cujo cobre ajuda a transportar oxig√™nio). Quer mais uma curiosidade?'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea3026a",
   "metadata": {},
   "source": [
    "# O que √© o pacote openai?\n",
    "\n",
    "Ele √© conhecido como uma biblioteca cliente em Python.\n",
    "\n",
    "Nada mais √© do que um inv√≥lucro para fazer exatamente esta chamada ao endpoint HTTP.\n",
    "\n",
    "Ele s√≥ permite que voc√™ trabalhe com c√≥digo Python arrumado em vez de ficar lidando com objetos JSON esquisitos.\n",
    "\n",
    "Mas √© s√≥ isso. Ele √© open-source e leve. Algumas pessoas acham que ele cont√©m c√≥digo de modelo da OpenAI - n√£o cont√©m!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "490fdf09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Claro! Fato divertido: os polvos t√™m tr√™s cora√ß√µes e sangue azul. Dois cora√ß√µes bombeiam sangue para as br√¢nquias, enquanto o terceiro bombeia para o resto do corpo. O sangue azul ocorre porque eles usam hemocianina (em vez de hemoglobina) para transportar o oxig√™nio. E, curiosamente, quando o polvo nada, o cora√ß√£o que envia sangue para o corpo para de bater, tornando a nata√ß√£o bem mais cansativa para ele ‚Äî por isso eles costumam se deslocar rastejando. Se quiser mais curiosidades, √© s√≥ pedir!'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Criar cliente OpenAI\n",
    "\n",
    "from openai import OpenAI\n",
    "openai = OpenAI()\n",
    "\n",
    "response = openai.chat.completions.create(model=\"gpt-5-nano\", messages=[{\"role\": \"user\", \"content\": \"Conte-me um fato divertido\"}])\n",
    "\n",
    "response.choices[0].message.content\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7739cda",
   "metadata": {},
   "source": [
    "## E ent√£o algo muito legal aconteceu:\n",
    "\n",
    "A API de Chat Completions da OpenAI ficou t√£o popular que outros provedores de modelo criaram endpoints id√™nticos.\n",
    "\n",
    "Eles s√£o conhecidos como \"OpenAI Compatible Endpoints\".\n",
    "\n",
    "Por exemplo, a Google fez um aqui: https://generativelanguage.googleapis.com/v1beta/openai/\n",
    "\n",
    "E a OpenAI decidiu ser gentil: eles disseram que voc√™ pode usar a mesma biblioteca cliente que fizemos para o GPT. Vamos permitir que voc√™ especifique um URL de endpoint diferente e uma chave diferente para usar outro provedor.\n",
    "\n",
    "Assim voc√™ pode usar:\n",
    "\n",
    "```python\n",
    "gemini = OpenAI(base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\", api_key=\"AIz....\")\n",
    "gemini.chat.completions.create(...)\n",
    "```\n",
    "\n",
    "E, s√≥ para deixar claro - mesmo que OpenAI apare√ßa no c√≥digo, estamos apenas usando essa biblioteca cliente leve em Python para chamar o endpoint - n√£o h√° nenhum modelo da OpenAI envolvido aqui.\n",
    "\n",
    "Se isso estiver confuso, revise o Guia 9 na pasta Guides!\n",
    "\n",
    "E agora vamos experimentar!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f74293bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chave de API encontrada e, at√© aqui, parece tudo certo!\n"
     ]
    }
   ],
   "source": [
    "GEMINI_BASE_URL = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "if not google_api_key:\n",
    "    print(\"Nenhuma chave de API foi encontrada - v√° at√© o notebook de solu√ß√£o de problemas nesta pasta para identificar e corrigir!\")\n",
    "elif not google_api_key.startswith(\"AIz\"):\n",
    "    print(\"Uma chave de API foi encontrada, mas ela n√£o come√ßa com AIz\")\n",
    "else:\n",
    "    print(\"Chave de API encontrada e, at√© aqui, parece tudo certo!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d060f484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Claro!\\n\\nAs lontras-do-mar dormem de m√£os dadas para n√£o se afastarem umas das outras enquanto flutuam na √°gua. √Äs vezes, um grupo inteiro tamb√©m se enrola em algas marinhas para que a correnteza n√£o os leve para longe.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemini = OpenAI(base_url=GEMINI_BASE_URL, api_key=google_api_key)\n",
    "\n",
    "response = gemini.chat.completions.create(model=\"gemini-2.5-pro\", messages=[{\"role\": \"user\", \"content\": \"Conte-me um fato divertido\"}])\n",
    "\n",
    "response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65272432",
   "metadata": {},
   "source": [
    "## E o Ollama tamb√©m oferece um endpoint compat√≠vel com OpenAI\n",
    "\n",
    "...e ele est√° na sua m√°quina local!\n",
    "\n",
    "Se a pr√≥xima c√©lula n√£o imprimir \"Ollama is running\" ent√£o abra um terminal e rode `ollama serve`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f06280ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Ollama is running'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.get(\"http://localhost:11434\").content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ef3807",
   "metadata": {},
   "source": [
    "### Baixe o llama3.2 da Meta\n",
    "\n",
    "Mude isto para llama3.2:1b se o seu computador for mais modesto.\n",
    "\n",
    "N√£o use llama3.3 ou llama4! Eles s√£o grandes demais para o seu computador..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e633481d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9419762",
   "metadata": {},
   "outputs": [],
   "source": [
    "OLLAMA_BASE_URL = \"http://localhost:11434/v1\"\n",
    "\n",
    "ollama = OpenAI(base_url=OLLAMA_BASE_URL, api_key='ollama')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2456cdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Vamos l√°!\\n\\nSabemos que a m√∫sica √© uma forma de express√£o poderosa, mas voc√™ j√° sabia pelo que a m√∫sica \"Malague√±a S√≥litaria\" do compositor espanhol Ernesto Lecuona foi associada?\\n\\nA ess√™ncia desse tema musical est√° presente em um filme antol√≥gico da Fox, \"O Futuro √© N√≥s\", de 2017. O filme conta a hist√≥ria de sua protagonista, Meryl Streep, que encontra o seu pr√≥prio passado atrav√©s de diferentes eras e pa√≠ses. E no meio desse show, ela acaba cantando a m√∫sica de Malague√±a S√≥litaria... Em uma cena, enquanto est√° se preparando para um evento no qual ela precisa ser \"nobre\" para uma reuni√£o; ou seja, sua performance com a m√∫sica √© t√£o bela que chega ao ponto onde o p√∫blico n√£o tem mais nada para fazer al√©m observar!'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obter um fato divertido\n",
    "\n",
    "response = ollama.chat.completions.create(model=\"llama3.2\", messages=[{\"role\": \"user\", \"content\": \"Conte-me um fato divertido\"}])\n",
    "\n",
    "response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6cae7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agora vamos testar deepseek-r1:1.5b - este √© o DeepSeek \"destilado\" em Qwen da Alibaba Cloud\n",
    "\n",
    "# !ollama pull deepseek-r1:1.5b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25002f25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Um fato divertido sobre ovos: \\n\\nVoc√™ sabia que ovos frescos podem flutuar em √°gua salgada quando est√£o a ponto de estragar? √â verdade! Quando os ovos envelhecem, a porose da casca permite que a densidade interna diminua, fazendo-os flutuar. J√° os ovos frescos s√£o mais densos e afundam na √°gua. üòÑ\\n\\n√â engra√ßado como o corpo humano usa pr√°ticas naturais de \"fertiliza√ß√£o\" mesmo antes de ser domesticado!\\n\\nTem outros fatos que eu poderia contar caso queira mais, voc√™ quer conhecer outro agora?'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#response = ollama.chat.completions.create(model=\"deepseek-r1:1.5b\", messages=[{\"role\": \"user\", \"content\": \"Conte-me um fato divertido\"}])\n",
    "response = ollama.chat.completions.create(model=\"deepseek-r1:latest\", messages=[{\"role\": \"user\", \"content\": \"Conte-me um fato divertido\"}])\n",
    "\n",
    "response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9fa1fc-eac5-4d1d-9be4-541b3f2b3458",
   "metadata": {},
   "source": [
    "# TAREFA DE CASA\n",
    "\n",
    "Atualize o projeto do dia 1 que resume uma p√°gina da web para usar um modelo open source rodando localmente via Ollama em vez da OpenAI\n",
    "\n",
    "Voc√™ poder√° usar essa t√©cnica em todos os projetos seguintes se preferir n√£o usar APIs pagas.\n",
    "\n",
    "**Benef√≠cios:**\n",
    "1. Sem cobran√ßas de API - open-source\n",
    "2. Os dados n√£o saem da sua m√°quina\n",
    "\n",
    "**Desvantagens:**\n",
    "1. Significativamente menos poder do que um modelo de fronteira\n",
    "\n",
    "## Recapitula√ß√£o da instala√ß√£o do Ollama\n",
    "\n",
    "Basta visitar [ollama.com](https://ollama.com) e instalar!\n",
    "\n",
    "Depois disso, o servidor do ollama j√° deve estar rodando localmente.  \n",
    "Se voc√™ visitar:  \n",
    "[http://localhost:11434/](http://localhost:11434/)\n",
    "\n",
    "Voc√™ deve ver a mensagem `Ollama is running`.  \n",
    "\n",
    "Se n√£o, abra um novo Terminal (Mac) ou Powershell (Windows) e execute `ollama serve`  \n",
    "E em outro Terminal (Mac) ou Powershell (Windows), execute `ollama pull llama3.2`  \n",
    "Depois tente [http://localhost:11434/](http://localhost:11434/) novamente.\n",
    "\n",
    "Se o Ollama estiver lento na sua m√°quina, tente usar `llama3.2:1b` como alternativa. Rode `ollama pull llama3.2:1b` em um Terminal ou Powershell e mude o c√≥digo de `MODEL = \"llama3.2\"` para `MODEL = \"llama3.2:1b\"`\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
